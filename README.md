# Bert-Pytorch
A pytorch implemetation of BERT model.

## How to use it ?
### Step 0: Prepare data
```bash
cd data
```
Training need a text file that store sentence pairs, which have format such as 'I like dogs \$\$\$ I also like cats'. By default, the file is named as 'bert_train_pairs.txt'.
```bash
python extract_word2idx.py extract
```
to extract mapping from word to index, which is saved in 'vocab.dat' by default.

The corpus I used is [The Blog Authorship Corpus](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm). Download and put it in folder 'raw', and type 
```bash
python extract_train_pairs.py extract
```
to generate the text file 'bert_train_pairs.txt' as mentioned above. 

### Step 1: Pretrain BERT model
```bash
cd ..
python train.py train
```
pretrain BERT model. 
The model state dict will be save in folder 'checkpoints/'. 
The information about model gragh, loss and accuracy for two tasks are saved in folder 'runs/', which is automatically generated by tensorboardX.


### Step 2: Finetune according to different tasks
```bash
python finetune.py finetune
```
Here to simplify, the task is next sentence prediction. The same dataset as in step 1 is used.

## EXperiments
Settings:
sentence paris: 1467976
vocab_size: 200004
parameters: 243077258
epochs: 8

